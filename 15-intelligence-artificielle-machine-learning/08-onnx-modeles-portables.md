üîù Retour au [Sommaire](/SOMMAIRE.md)

# 15.8 ONNX et mod√®les portables

## Introduction √† ONNX

ONNX (Open Neural Network Exchange) est un **format ouvert** pour repr√©senter des mod√®les d'apprentissage automatique. C'est un peu comme un "PDF pour l'intelligence artificielle" : un format universel que tout le monde peut lire.

### Qu'est-ce qu'ONNX ?

ONNX permet de :
- **Entra√Æner** un mod√®le dans un framework (PyTorch, TensorFlow, scikit-learn)
- **Exporter** le mod√®le au format ONNX
- **Utiliser** ce mod√®le dans n'importe quel langage/plateforme
- **D√©ployer** sur mobile, serveur, embarqu√©, desktop

### Pourquoi ONNX est important ?

**Sans ONNX :**
```
Entra√Ænement PyTorch ‚Üí Bloqu√© dans PyTorch
                      ‚Üí N√©cessite Python en production
                      ‚Üí D√©pendances lourdes
```

**Avec ONNX :**
```
Entra√Ænement PyTorch ‚Üí Export ONNX ‚Üí Utilisation FreePascal
                                   ‚Üí Utilisation C++
                                   ‚Üí Utilisation JavaScript
                                   ‚Üí Utilisation mobile
```

### Applications concr√®tes

üéØ **Reconnaissance d'images**
- Entra√Æner avec PyTorch sur GPU puissant
- Exporter en ONNX
- D√©ployer dans une application Lazarus sur PC bas de gamme

üó£Ô∏è **Traitement du langage**
- Entra√Æner un mod√®le de sentiment avec TensorFlow
- Exporter en ONNX
- Int√©grer dans une application FreePascal

ü§ñ **Vision par ordinateur**
- Utiliser un mod√®le pr√©-entra√Æn√© (YOLO, ResNet)
- Convertir en ONNX
- Utiliser dans une app de surveillance avec Lazarus

### Avantages pour FreePascal

‚úÖ **Performance**
- Ex√©cution optimis√©e (CPU/GPU)
- Pas d'interpr√©teur Python
- Code natif rapide

‚úÖ **D√©ploiement**
- Un seul fichier .onnx √† distribuer
- Pas de d√©pendances Python
- Fonctionne hors ligne

‚úÖ **Portabilit√©**
- Windows, Linux, macOS
- M√™me code, m√™me mod√®le
- Architecture x86/ARM

---

## Architecture et Composants

### Structure d'un mod√®le ONNX

Un fichier ONNX contient :

```
fichier.onnx
‚îú‚îÄ‚îÄ Graphe du mod√®le
‚îÇ   ‚îú‚îÄ‚îÄ N≈ìuds (op√©rations)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Convolution
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ ReLU
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ MaxPool
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ Dense
‚îÇ   ‚îú‚îÄ‚îÄ Entr√©es
‚îÇ   ‚îî‚îÄ‚îÄ Sorties
‚îú‚îÄ‚îÄ Poids (weights)
‚îú‚îÄ‚îÄ M√©tadonn√©es
‚îÇ   ‚îú‚îÄ‚îÄ Version
‚îÇ   ‚îú‚îÄ‚îÄ Producteur
‚îÇ   ‚îî‚îÄ‚îÄ Description
‚îî‚îÄ‚îÄ Types de donn√©es
```

### ONNX Runtime

**ONNX Runtime** est le moteur d'ex√©cution qui lit les fichiers .onnx et effectue les pr√©dictions.

**Caract√©ristiques :**
- D√©velopp√© par Microsoft
- Open source
- Tr√®s optimis√© (SIMD, multi-threading)
- Support CPU et GPU
- Disponible en C/C++ (interfa√ßable avec Pascal)

---

## Installation et Configuration

### Installation d'ONNX Runtime sur Windows

**M√©thode 1 : T√©l√©chargement manuel**

1. Aller sur https://github.com/microsoft/onnxruntime/releases
2. T√©l√©charger `onnxruntime-win-x64-[version].zip`
3. Extraire dans `C:\onnxruntime\`

**Structure des fichiers :**
```
C:\onnxruntime\
‚îú‚îÄ‚îÄ bin\
‚îÇ   ‚îî‚îÄ‚îÄ onnxruntime.dll
‚îú‚îÄ‚îÄ include\
‚îÇ   ‚îú‚îÄ‚îÄ onnxruntime_c_api.h
‚îÇ   ‚îî‚îÄ‚îÄ ...
‚îî‚îÄ‚îÄ lib\
    ‚îî‚îÄ‚îÄ onnxruntime.lib
```

**M√©thode 2 : Via package manager**

```batch
choco install onnxruntime
```

### Installation d'ONNX Runtime sur Linux/Ubuntu

**Via package manager :**

```bash
# Installer les d√©pendances
sudo apt update
sudo apt install build-essential

# T√©l√©charger ONNX Runtime
wget https://github.com/microsoft/onnxruntime/releases/download/v1.16.0/onnxruntime-linux-x64-1.16.0.tgz

# Extraire
tar -xzf onnxruntime-linux-x64-1.16.0.tgz

# Installer
sudo cp -r onnxruntime-linux-x64-1.16.0/lib/* /usr/local/lib/
sudo cp -r onnxruntime-linux-x64-1.16.0/include/* /usr/local/include/
sudo ldconfig
```

### Configuration FreePascal

**V√©rifier l'installation :**

```pascal
program TestONNXInstall;

{$mode objfpc}{$H+}

uses
  SysUtils, dynlibs;

var
  LibHandle: TLibHandle;

begin
  {$IFDEF WINDOWS}
  LibHandle := LoadLibrary('C:\onnxruntime\bin\onnxruntime.dll');
  {$ENDIF}
  {$IFDEF LINUX}
  LibHandle := LoadLibrary('libonnxruntime.so');
  {$ENDIF}

  if LibHandle <> 0 then
  begin
    WriteLn('‚úì ONNX Runtime d√©tect√© et charg√© avec succ√®s!');
    FreeLibrary(LibHandle);
  end
  else
    WriteLn('‚úó ONNX Runtime introuvable');

  {$IFDEF WINDOWS}
  ReadLn;
  {$ENDIF}
end.
```

---

## Bindings ONNX Runtime pour FreePascal

### Interface de base

```pascal
unit ONNXRuntime;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, dynlibs;

const
  {$IFDEF WINDOWS}
  ONNX_LIB = 'onnxruntime.dll';
  {$ENDIF}
  {$IFDEF LINUX}
  ONNX_LIB = 'libonnxruntime.so';
  {$ENDIF}
  {$IFDEF DARWIN}
  ONNX_LIB = 'libonnxruntime.dylib';
  {$ENDIF}

type
  // Types opaques (pointeurs)
  POrtEnv = Pointer;
  POrtSession = Pointer;
  POrtSessionOptions = Pointer;
  POrtValue = Pointer;
  POrtStatus = Pointer;
  POrtAllocator = Pointer;
  POrtApi = Pointer;

  // Type de donn√©e
  ONNXTensorElementDataType = (
    ONNX_TENSOR_ELEMENT_DATA_TYPE_UNDEFINED = 0,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT = 1,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT8 = 2,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_INT8 = 3,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_UINT16 = 4,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_INT16 = 5,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_INT32 = 6,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_INT64 = 7,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_STRING = 8,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_BOOL = 9,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT16 = 10,
    ONNX_TENSOR_ELEMENT_DATA_TYPE_DOUBLE = 11
  );

  // Code d'erreur
  OrtErrorCode = (
    ORT_OK = 0,
    ORT_FAIL = 1,
    ORT_INVALID_ARGUMENT = 2,
    ORT_NO_SUCHFILE = 3,
    ORT_NO_MODEL = 4,
    ORT_ENGINE_ERROR = 5,
    ORT_RUNTIME_EXCEPTION = 6,
    ORT_INVALID_PROTOBUF = 7,
    ORT_MODEL_LOADED = 8,
    ORT_NOT_IMPLEMENTED = 9,
    ORT_INVALID_GRAPH = 10,
    ORT_EP_FAIL = 11
  );

  // API principale
  TOrtApi = record
    CreateEnv: function(log_severity_level: Integer; logid: PChar;
                        out env: POrtEnv): POrtStatus; cdecl;
    CreateSession: function(env: POrtEnv; model_path: PChar;
                            options: POrtSessionOptions;
                            out session: POrtSession): POrtStatus; cdecl;
    CreateSessionOptions: function(out options: POrtSessionOptions): POrtStatus; cdecl;
    CreateCpuMemoryInfo: function(atype: Integer; mem_type: Integer;
                                   out info: Pointer): POrtStatus; cdecl;
    CreateTensorWithDataAsOrtValue: function(info: Pointer;
                                             p_data: Pointer;
                                             p_data_len: NativeUInt;
                                             const shape: PInt64;
                                             shape_len: NativeUInt;
                                             data_type: ONNXTensorElementDataType;
                                             out value: POrtValue): POrtStatus; cdecl;
    Run: function(session: POrtSession; run_options: Pointer;
                  const input_names: PPChar; const inputs: PPOrtValue;
                  input_len: NativeUInt;
                  const output_names: PPChar; output_names_len: NativeUInt;
                  out outputs: PPOrtValue): POrtStatus; cdecl;
    GetTensorMutableData: function(value: POrtValue; out data: Pointer): POrtStatus; cdecl;
    ReleaseValue: procedure(value: POrtValue); cdecl;
    ReleaseSession: procedure(session: POrtSession); cdecl;
    ReleaseSessionOptions: procedure(options: POrtSessionOptions); cdecl;
    ReleaseEnv: procedure(env: POrtEnv); cdecl;
    ReleaseStatus: procedure(status: POrtStatus); cdecl;
    GetErrorMessage: function(status: POrtStatus): PChar; cdecl;
    GetErrorCode: function(status: POrtStatus): OrtErrorCode; cdecl;
  end;

var
  OrtApi: ^TOrtApi;
  OrtLibHandle: TLibHandle;

function LoadONNXRuntime: Boolean;
procedure UnloadONNXRuntime;
function CheckStatus(status: POrtStatus): Boolean;

implementation

function LoadONNXRuntime: Boolean;
type
  TGetApiBaseFunc = function: Pointer; cdecl;
var
  GetApiBase: TGetApiBaseFunc;
  ApiBase: Pointer;
begin
  Result := False;

  OrtLibHandle := LoadLibrary(ONNX_LIB);
  if OrtLibHandle = 0 then
  begin
    WriteLn('Erreur: Impossible de charger ', ONNX_LIB);
    Exit;
  end;

  GetApiBase := TGetApiBaseFunc(GetProcAddress(OrtLibHandle, 'OrtGetApiBase'));
  if not Assigned(GetApiBase) then
  begin
    WriteLn('Erreur: OrtGetApiBase non trouv√©e');
    FreeLibrary(OrtLibHandle);
    Exit;
  end;

  ApiBase := GetApiBase();
  if ApiBase = nil then
  begin
    WriteLn('Erreur: API Base nulle');
    FreeLibrary(OrtLibHandle);
    Exit;
  end;

  // R√©cup√©rer l'API version 1
  OrtApi := Pointer(PPointer(ApiBase + SizeOf(Pointer))^);

  Result := Assigned(OrtApi);
end;

procedure UnloadONNXRuntime;
begin
  if OrtLibHandle <> 0 then
  begin
    FreeLibrary(OrtLibHandle);
    OrtLibHandle := 0;
  end;
end;

function CheckStatus(status: POrtStatus): Boolean;
var
  errorMsg: PChar;
  errorCode: OrtErrorCode;
begin
  if status = nil then
  begin
    Result := True;
    Exit;
  end;

  Result := False;
  errorCode := OrtApi^.GetErrorCode(status);

  if errorCode <> ORT_OK then
  begin
    errorMsg := OrtApi^.GetErrorMessage(status);
    WriteLn('Erreur ONNX: ', string(errorMsg));
  end;

  OrtApi^.ReleaseStatus(status);
end;

initialization
  OrtLibHandle := 0;

finalization
  UnloadONNXRuntime;

end.
```

---

## Premier Exemple : Chargement d'un Mod√®le

### Cr√©er un mod√®le simple avec Python

D'abord, cr√©ons un mod√®le simple pour tester :

```python
# create_simple_model.py
import torch
import torch.nn as nn

class SimpleModel(nn.Module):
    def __init__(self):
        super(SimpleModel, self).__init__()
        self.linear = nn.Linear(3, 1)

    def forward(self, x):
        return self.linear(x)

# Cr√©er et entra√Æner
model = SimpleModel()
model.eval()

# Exemple d'entr√©e
dummy_input = torch.randn(1, 3)

# Exporter en ONNX
torch.onnx.export(
    model,
    dummy_input,
    "simple_model.onnx",
    input_names=['input'],
    output_names=['output'],
    dynamic_axes={'input': {0: 'batch_size'}}
)

print("Mod√®le export√© : simple_model.onnx")
```

### Charger et utiliser le mod√®le en FreePascal

```pascal
program LoadSimpleModel;

{$mode objfpc}{$H+}

uses
  SysUtils, ONNXRuntime;

var
  env: POrtEnv;
  sessionOptions: POrtSessionOptions;
  session: POrtSession;
  status: POrtStatus;
  modelPath: string;

begin
  WriteLn('=== Chargement d''un mod√®le ONNX ===');
  WriteLn;

  if not LoadONNXRuntime then
  begin
    WriteLn('√âchec du chargement d''ONNX Runtime');
    Exit;
  end;

  try
    // Cr√©er l'environnement
    status := OrtApi^.CreateEnv(3, 'test', env);
    if not CheckStatus(status) then
    begin
      WriteLn('√âchec de cr√©ation de l''environnement');
      Exit;
    end;

    WriteLn('‚úì Environnement cr√©√©');

    // Cr√©er les options de session
    status := OrtApi^.CreateSessionOptions(sessionOptions);
    if not CheckStatus(status) then
    begin
      OrtApi^.ReleaseEnv(env);
      Exit;
    end;

    WriteLn('‚úì Options de session cr√©√©es');

    // Charger le mod√®le
    modelPath := 'simple_model.onnx';

    {$IFDEF WINDOWS}
    status := OrtApi^.CreateSession(env, PChar(UTF8String(modelPath)),
                                     sessionOptions, session);
    {$ELSE}
    status := OrtApi^.CreateSession(env, PChar(modelPath),
                                     sessionOptions, session);
    {$ENDIF}

    if not CheckStatus(status) then
    begin
      WriteLn('√âchec du chargement du mod√®le');
      OrtApi^.ReleaseSessionOptions(sessionOptions);
      OrtApi^.ReleaseEnv(env);
      Exit;
    end;

    WriteLn('‚úì Mod√®le charg√© avec succ√®s: ', modelPath);
    WriteLn;
    WriteLn('Le mod√®le est pr√™t pour l''inf√©rence!');

    // Lib√©rer les ressources
    OrtApi^.ReleaseSession(session);
    OrtApi^.ReleaseSessionOptions(sessionOptions);
    OrtApi^.ReleaseEnv(env);

  finally
    UnloadONNXRuntime;
  end;

  {$IFDEF WINDOWS}
  WriteLn;
  WriteLn('Appuyez sur Entr√©e pour quitter...');
  ReadLn;
  {$ENDIF}
end.
```

---

## Inf√©rence : Faire des Pr√©dictions

### Wrapper pour faciliter l'utilisation

```pascal
unit ONNXInference;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, ONNXRuntime;

type
  TONNXModel = class
  private
    FEnv: POrtEnv;
    FSession: POrtSession;
    FSessionOptions: POrtSessionOptions;
    FAllocator: POrtAllocator;
    FInputName: string;
    FOutputName: string;
    FLoaded: Boolean;
  public
    constructor Create;
    destructor Destroy; override;

    function LoadModel(const AModelPath: string): Boolean;
    function Predict(const AInputData: array of Single): TArray<Single>;
    function PredictBatch(const AInputData: array of Single;
                          ABatchSize, AInputSize: Integer): TArray<Single>;

    property Loaded: Boolean read FLoaded;
  end;

implementation

constructor TONNXModel.Create;
begin
  FLoaded := False;
  FInputName := 'input';
  FOutputName := 'output';
end;

destructor TONNXModel.Destroy;
begin
  if FLoaded then
  begin
    if Assigned(FSession) then
      OrtApi^.ReleaseSession(FSession);
    if Assigned(FSessionOptions) then
      OrtApi^.ReleaseSessionOptions(FSessionOptions);
    if Assigned(FEnv) then
      OrtApi^.ReleaseEnv(FEnv);
  end;

  inherited;
end;

function TONNXModel.LoadModel(const AModelPath: string): Boolean;
var
  status: POrtStatus;
begin
  Result := False;

  if not LoadONNXRuntime then
  begin
    WriteLn('Erreur: ONNX Runtime non disponible');
    Exit;
  end;

  // Cr√©er l'environnement
  status := OrtApi^.CreateEnv(3, 'inference', FEnv);
  if not CheckStatus(status) then
    Exit;

  // Cr√©er les options
  status := OrtApi^.CreateSessionOptions(FSessionOptions);
  if not CheckStatus(status) then
  begin
    OrtApi^.ReleaseEnv(FEnv);
    Exit;
  end;

  // Charger le mod√®le
  {$IFDEF WINDOWS}
  status := OrtApi^.CreateSession(FEnv, PChar(UTF8String(AModelPath)),
                                   FSessionOptions, FSession);
  {$ELSE}
  status := OrtApi^.CreateSession(FEnv, PChar(AModelPath),
                                   FSessionOptions, FSession);
  {$ENDIF}

  if not CheckStatus(status) then
  begin
    OrtApi^.ReleaseSessionOptions(FSessionOptions);
    OrtApi^.ReleaseEnv(FEnv);
    Exit;
  end;

  FLoaded := True;
  Result := True;
end;

function TONNXModel.Predict(const AInputData: array of Single): TArray<Single>;
var
  status: POrtStatus;
  memoryInfo: Pointer;
  inputTensor, outputTensor: POrtValue;
  inputShape: array[0..1] of Int64;
  outputData: PSingle;
  inputNames, outputNames: array[0..0] of PChar;
  inputValues, outputValues: array[0..0] of POrtValue;
  i: Integer;
begin
  SetLength(Result, 0);

  if not FLoaded then
  begin
    WriteLn('Erreur: Mod√®le non charg√©');
    Exit;
  end;

  // Cr√©er l'info m√©moire CPU
  status := OrtApi^.CreateCpuMemoryInfo(0, 0, memoryInfo);
  if not CheckStatus(status) then
    Exit;

  try
    // Forme de l'entr√©e: [1, Length(AInputData)]
    inputShape[0] := 1;
    inputShape[1] := Length(AInputData);

    // Cr√©er le tenseur d'entr√©e
    status := OrtApi^.CreateTensorWithDataAsOrtValue(
      memoryInfo,
      @AInputData[0],
      Length(AInputData) * SizeOf(Single),
      @inputShape[0],
      2,
      ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT,
      inputTensor
    );

    if not CheckStatus(status) then
      Exit;

    try
      // Pr√©parer les noms
      inputNames[0] := PChar(FInputName);
      outputNames[0] := PChar(FOutputName);
      inputValues[0] := inputTensor;

      // Ex√©cuter l'inf√©rence
      status := OrtApi^.Run(
        FSession,
        nil,
        @inputNames[0],
        @inputValues[0],
        1,
        @outputNames[0],
        1,
        @outputValues[0]
      );

      if not CheckStatus(status) then
        Exit;

      outputTensor := outputValues[0];

      try
        // R√©cup√©rer les donn√©es de sortie
        status := OrtApi^.GetTensorMutableData(outputTensor, Pointer(outputData));
        if not CheckStatus(status) then
          Exit;

        // Copier le r√©sultat (supposons 1 sortie pour l'exemple)
        SetLength(Result, 1);
        Result[0] := outputData^;

      finally
        OrtApi^.ReleaseValue(outputTensor);
      end;

    finally
      OrtApi^.ReleaseValue(inputTensor);
    end;

  finally
    // Note: memoryInfo n'a pas besoin d'√™tre lib√©r√© explicitement
  end;
end;

function TONNXModel.PredictBatch(const AInputData: array of Single;
                                  ABatchSize, AInputSize: Integer): TArray<Single>;
begin
  // √Ä impl√©menter selon vos besoins
  SetLength(Result, 0);
end;

end.
```

### Exemple d'utilisation

```pascal
program SimpleInference;

{$mode objfpc}{$H+}

uses
  SysUtils, ONNXInference;

var
  model: TONNXModel;
  input: array[0..2] of Single;
  output: TArray<Single>;

begin
  WriteLn('=== Inf√©rence ONNX ===');
  WriteLn;

  model := TONNXModel.Create;
  try
    // Charger le mod√®le
    if not model.LoadModel('simple_model.onnx') then
    begin
      WriteLn('√âchec du chargement du mod√®le');
      Exit;
    end;

    WriteLn('‚úì Mod√®le charg√©');
    WriteLn;

    // Pr√©parer l'entr√©e
    input[0] := 1.0;
    input[1] := 2.0;
    input[2] := 3.0;

    WriteLn('Entr√©e: [', input[0]:0:2, ', ', input[1]:0:2, ', ', input[2]:0:2, ']');

    // Faire la pr√©diction
    output := model.Predict(input);

    if Length(output) > 0 then
    begin
      WriteLn('Sortie: ', output[0]:0:4);
    end
    else
      WriteLn('√âchec de la pr√©diction');

  finally
    model.Free;
  end;

  {$IFDEF WINDOWS}
  WriteLn;
  WriteLn('Appuyez sur Entr√©e pour quitter...');
  ReadLn;
  {$ENDIF}
end.
```

---

## Application Pratique : Classification d'Images

### Cr√©er un mod√®le de classification avec PyTorch

```python
# create_mnist_model.py
import torch
import torch.nn as nn
import torchvision
import torchvision.transforms as transforms

class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(1, 16, 3, padding=1)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.fc1 = nn.Linear(32 * 7 * 7, 128)
        self.fc2 = nn.Linear(128, 10)
        self.pool = nn.MaxPool2d(2, 2)
        self.relu = nn.ReLU()

    def forward(self, x):
        x = self.pool(self.relu(self.conv1(x)))
        x = self.pool(self.relu(self.conv2(x)))
        x = x.view(-1, 32 * 7 * 7)
        x = self.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# Cr√©er le mod√®le
model = SimpleCNN()
model.eval()

# Exporter
dummy_input = torch.randn(1, 1, 28, 28)
torch.onnx.export(
    model,
    dummy_input,
    "mnist_model.onnx",
    input_names=['image'],
    output_names=['logits'],
    dynamic_axes={'image': {0: 'batch_size'}}
)

print("Mod√®le MNIST export√©!")
```

### Utilisation en FreePascal

```pascal
program MNISTClassifier;

{$mode objfpc}{$H+}

uses
  SysUtils, Classes, ONNXInference, Graphics;

type
  TImageClassifier = class
  private
    FModel: TONNXModel;
    FLabels: TStringList;
  public
    constructor Create;
    destructor Destroy; override;

    function LoadModel(const AModelPath: string;
                       const ALabelsFile: string): Boolean;
    function ClassifyImage(const AImagePath: string): string;
    function PreprocessImage(ABitmap: TBitmap): TArray<Single>;
  end;

constructor TImageClassifier.Create;
begin
  FModel := TONNXModel.Create;
  FLabels := TStringList.Create;
end;

destructor TImageClassifier.Destroy;
begin
  FModel.Free;
  FLabels.Free;
  inherited;
end;

function TImageClassifier.LoadModel(const AModelPath: string;
                                     const ALabelsFile: string): Boolean;
begin
  Result := False;

  if not FModel.LoadModel(AModelPath) then
  begin
    WriteLn('√âchec du chargement du mod√®le');
    Exit;
  end;

  if FileExists(ALabelsFile) then
    FLabels.LoadFromFile(ALabelsFile)
  else
  begin
    // Labels par d√©faut pour MNIST
    FLabels.Add('0');
    FLabels.Add('1');
    FLabels.Add('2');
    FLabels.Add('3');
    FLabels.Add('4');
    FLabels.Add('5');
    FLabels.Add('6');
    FLabels.Add('7');
    FLabels.Add('8');
    FLabels.Add('9');
  end;

  Result := True;
end;

function TImageClassifier.PreprocessImage(ABitmap: TBitmap): TArray<Single>;
var
  x, y: Integer;
  pixel: TColor;
  grayValue: Single;
begin
  SetLength(Result, 28 * 28);

  // Redimensionner si n√©cessaire
  if (ABitmap.Width <> 28) or (ABitmap.Height <> 28) then
  begin
    WriteLn('Note: L''image devrait √™tre 28x28');
    // TODO: Impl√©menter le redimensionnement
  end;

  // Convertir en niveaux de gris et normaliser
  for y := 0 to 27 do
  begin
    for x := 0 to 27 do
    begin
      pixel := ABitmap.Canvas.Pixels[x, y];

      // Convertir en niveau de gris
      grayValue := (Red(pixel) + Green(pixel) + Blue(pixel)) / 3.0;

      // Normaliser [0, 255] ‚Üí [0, 1]
      Result[y * 28 + x] := grayValue / 255.0;
    end;
  end;
end;

function TImageClassifier.ClassifyImage(const AImagePath: string): string;
var
  bitmap: TBitmap;
  inputData, output: TArray<Single>;
  maxIndex: Integer;
  i: Integer;
  maxValue: Single;
begin
  Result := 'Erreur';

  bitmap := TBitmap.Create;
  try
    bitmap.LoadFromFile(AImagePath);

    // Pr√©traiter l'image
    inputData := PreprocessImage(bitmap);

    // Faire la pr√©diction
    output := FModel.Predict(inputData);

    if Length(output) = 0 then
    begin
      WriteLn('√âchec de la pr√©diction');
      Exit;
    end;

    // Trouver la classe avec le score le plus √©lev√©
    maxIndex := 0;
    maxValue := output[0];

    for i := 1 to High(output) do
    begin
      if output[i] > maxValue then
      begin
        maxValue := output[i];
        maxIndex := i;
      end;
    end;

    // Retourner le label correspondant
    if (maxIndex >= 0) and (maxIndex < FLabels.Count) then
      Result := FLabels[maxIndex]
    else
      Result := IntToStr(maxIndex);

  finally
    bitmap.Free;
  end;
end;

var
  classifier: TImageClassifier;
  result: string;

begin
  WriteLn('=== Classification d''image MNIST ===');
  WriteLn;

  classifier := TImageClassifier.Create;
  try
    if not classifier.LoadModel('mnist_model.onnx', 'labels.txt') then
    begin
      WriteLn('√âchec du chargement');
      Exit;
    end;

    WriteLn('‚úì Mod√®le charg√©');
    WriteLn;

    // Classifier une image
    result := classifier.ClassifyImage('digit_test.bmp');
    WriteLn('Pr√©diction: ', result);

  finally
    classifier.Free;
  end;

  {$IFDEF WINDOWS}
  WriteLn;
  WriteLn('Appuyez sur Entr√©e pour quitter...');
  ReadLn;
  {$ENDIF}
end.
```

---

## Mod√®les Pr√©-Entra√Æn√©s Populaires

### ONNX Model Zoo

Microsoft maintient un d√©p√¥t de mod√®les pr√©-entra√Æn√©s au format ONNX :
https://github.com/onnx/models

**Cat√©gories disponibles :**

üñºÔ∏è **Vision par ordinateur**
- ResNet (classification d'images)
- MobileNet (optimis√© mobile)
- YOLO (d√©tection d'objets)
- SqueezeNet (tr√®s l√©ger)

üìù **Traitement du langage**
- BERT (compr√©hension du texte)
- GPT-2 (g√©n√©ration de texte)
- RoBERTa (analyse de sentiment)

üó£Ô∏è **Audio**
- Wav2Vec (reconnaissance vocale)
- DeepSpeech (speech-to-text)

### T√©l√©charger et utiliser ResNet

```pascal
program UseResNet;

{$mode objfpc}{$H+}

uses
  SysUtils, Classes, ONNXInference, Graphics;

const
  // Classes ImageNet (1000 cat√©gories)
  IMAGENET_CLASSES: array[0..4] of string = (
    'tench, Tinca tinca',
    'goldfish, Carassius auratus',
    'great white shark, white shark',
    'tiger shark, Galeocerdo cuvieri',
    'hammerhead, hammerhead shark'
    // ... (1000 classes au total)
  );

type
  TResNetClassifier = class
  private
    FModel: TONNXModel;
    function PreprocessImageNet(ABitmap: TBitmap): TArray<Single>;
    function Softmax(const ALogits: TArray<Single>): TArray<Single>;
  public
    constructor Create;
    destructor Destroy; override;

    function LoadModel: Boolean;
    function ClassifyImage(const AImagePath: string;
                          ATopK: Integer = 5): TStringList;
  end;

constructor TResNetClassifier.Create;
begin
  FModel := TONNXModel.Create;
end;

destructor TResNetClassifier.Destroy;
begin
  FModel.Free;
  inherited;
end;

function TResNetClassifier.LoadModel: Boolean;
begin
  // ResNet-50 t√©l√©charg√© depuis ONNX Model Zoo
  Result := FModel.LoadModel('resnet50-v2-7.onnx');
end;

function TResNetClassifier.PreprocessImageNet(ABitmap: TBitmap): TArray<Single>;
var
  x, y, c: Integer;
  pixel: TColor;
  r, g, b: Single;
  mean, std: array[0..2] of Single;
begin
  // Normalisation ImageNet
  mean[0] := 0.485; mean[1] := 0.456; mean[2] := 0.406;
  std[0] := 0.229; std[1] := 0.224; std[2] := 0.225;

  // ResNet attend 224x224x3 (RGB)
  SetLength(Result, 3 * 224 * 224);

  // TODO: Redimensionner √† 224x224 si n√©cessaire

  for y := 0 to 223 do
  begin
    for x := 0 to 223 do
    begin
      pixel := ABitmap.Canvas.Pixels[x, y];

      // Extraire les canaux RGB
      r := Red(pixel) / 255.0;
      g := Green(pixel) / 255.0;
      b := Blue(pixel) / 255.0;

      // Normaliser avec mean et std
      r := (r - mean[0]) / std[0];
      g := (g - mean[1]) / std[1];
      b := (b - mean[2]) / std[2];

      // Format CHW (Channel, Height, Width)
      Result[0 * 224 * 224 + y * 224 + x] := r;
      Result[1 * 224 * 224 + y * 224 + x] := g;
      Result[2 * 224 * 224 + y * 224 + x] := b;
    end;
  end;
end;

function TResNetClassifier.Softmax(const ALogits: TArray<Single>): TArray<Single>;
var
  i: Integer;
  maxLogit, sumExp: Single;
begin
  SetLength(Result, Length(ALogits));

  // Trouver le maximum pour la stabilit√© num√©rique
  maxLogit := ALogits[0];
  for i := 1 to High(ALogits) do
    if ALogits[i] > maxLogit then
      maxLogit := ALogits[i];

  // Calculer exp et somme
  sumExp := 0;
  for i := 0 to High(ALogits) do
  begin
    Result[i] := Exp(ALogits[i] - maxLogit);
    sumExp := sumExp + Result[i];
  end;

  // Normaliser
  for i := 0 to High(Result) do
    Result[i] := Result[i] / sumExp;
end;

function TResNetClassifier.ClassifyImage(const AImagePath: string;
                                          ATopK: Integer): TStringList;
type
  TClassScore = record
    Index: Integer;
    Score: Single;
  end;
var
  bitmap: TBitmap;
  inputData, logits, probabilities: TArray<Single>;
  scores: array of TClassScore;
  i, j: Integer;
  temp: TClassScore;
begin
  Result := TStringList.Create;

  bitmap := TBitmap.Create;
  try
    bitmap.LoadFromFile(AImagePath);

    // Pr√©traiter
    inputData := PreprocessImageNet(bitmap);

    // Pr√©diction
    logits := FModel.Predict(inputData);

    if Length(logits) = 0 then
    begin
      Result.Add('Erreur de pr√©diction');
      Exit;
    end;

    // Convertir en probabilit√©s
    probabilities := Softmax(logits);

    // Cr√©er un tableau de scores
    SetLength(scores, Length(probabilities));
    for i := 0 to High(probabilities) do
    begin
      scores[i].Index := i;
      scores[i].Score := probabilities[i];
    end;

    // Trier par score d√©croissant (tri √† bulles)
    for i := 0 to Length(scores) - 2 do
      for j := i + 1 to Length(scores) - 1 do
        if scores[j].Score > scores[i].Score then
        begin
          temp := scores[i];
          scores[i] := scores[j];
          scores[j] := temp;
        end;

    // Retourner les TopK
    for i := 0 to Min(ATopK - 1, High(scores)) do
    begin
      Result.Add(Format('%s (%.2f%%)',
        [IMAGENET_CLASSES[scores[i].Index], scores[i].Score * 100]));
    end;

  finally
    bitmap.Free;
  end;
end;

end.
```

---

## Optimisation des Performances

### 1. Quantification (Quantization)

R√©duire la pr√©cision des poids pour acc√©l√©rer l'inf√©rence :

**Python - Quantifier un mod√®le :**
```python
import onnx
from onnxruntime.quantization import quantize_dynamic, QuantType

model_fp32 = 'model.onnx'
model_quant = 'model_quantized.onnx'

quantize_dynamic(
    model_fp32,
    model_quant,
    weight_type=QuantType.QUInt8
)

print("Mod√®le quantifi√© cr√©√©!")
print(f"Taille originale: {os.path.getsize(model_fp32) / 1024 / 1024:.2f} MB")
print(f"Taille quantifi√©e: {os.path.getsize(model_quant) / 1024 / 1024:.2f} MB")
```

**Avantages :**
- ‚úÖ Mod√®le 2-4√ó plus petit
- ‚úÖ Inf√©rence 2-4√ó plus rapide
- ‚úÖ Moins de m√©moire
- ‚ö†Ô∏è L√©g√®re perte de pr√©cision (g√©n√©ralement <1%)

### 2. Configuration des threads

```pascal
procedure OptimizeSessionOptions(var AOptions: POrtSessionOptions);
var
  status: POrtStatus;
begin
  // Utiliser tous les c≈ìurs CPU disponibles
  status := OrtApi^.SetIntraOpNumThreads(AOptions, 0); // 0 = auto
  CheckStatus(status);

  // Optimisations du graphe
  status := OrtApi^.SetSessionGraphOptimizationLevel(AOptions, 3); // Max
  CheckStatus(status);
end;
```

### 3. Batch Processing

Traiter plusieurs images en une seule fois :

```pascal
function PredictBatch(const AImages: array of TBitmap): TArray<TArray<Single>>;
var
  batchSize, i: Integer;
  inputData: TArray<Single>;
  output: TArray<Single>;
begin
  batchSize := Length(AImages);

  // Pr√©parer les donn√©es batch: [batchSize, channels, height, width]
  SetLength(inputData, batchSize * 3 * 224 * 224);

  for i := 0 to batchSize - 1 do
  begin
    // Pr√©traiter chaque image et ajouter au batch
    PreprocessImageToBatch(AImages[i], inputData, i);
  end;

  // Une seule inf√©rence pour tout le batch
  output := FModel.PredictBatch(inputData, batchSize, 3 * 224 * 224);

  // S√©parer les r√©sultats
  Result := SplitBatchOutput(output, batchSize);
end;
```

### 4. Utilisation du GPU

**Activer CUDA (si disponible) :**

```pascal
procedure EnableCUDA(var ASessionOptions: POrtSessionOptions);
var
  status: POrtStatus;
begin
  // Essayer d'activer CUDA
  status := OrtApi^.SessionOptionsAppendExecutionProvider_CUDA(ASessionOptions, 0);

  if CheckStatus(status) then
    WriteLn('‚úì GPU CUDA activ√©')
  else
    WriteLn('‚Ñπ GPU non disponible, utilisation du CPU');
end;
```

---

## Gestion Multi-Plateforme

### Chemins des biblioth√®ques

```pascal
unit ONNXPlatform;

{$mode objfpc}{$H+}

interface

uses
  SysUtils;

function GetONNXRuntimePath: string;
function GetModelsPath: string;
procedure EnsureDirectories;

implementation

function GetONNXRuntimePath: string;
begin
  {$IFDEF WINDOWS}
  Result := 'C:\onnxruntime\bin\onnxruntime.dll';
  {$ENDIF}

  {$IFDEF LINUX}
  // Chercher dans plusieurs emplacements
  if FileExists('/usr/local/lib/libonnxruntime.so') then
    Result := '/usr/local/lib/libonnxruntime.so'
  else if FileExists('/usr/lib/libonnxruntime.so') then
    Result := '/usr/lib/libonnxruntime.so'
  else
    Result := 'libonnxruntime.so'; // Chercher dans LD_LIBRARY_PATH
  {$ENDIF}

  {$IFDEF DARWIN}
  Result := '/usr/local/lib/libonnxruntime.dylib';
  {$ENDIF}
end;

function GetModelsPath: string;
begin
  {$IFDEF WINDOWS}
  Result := ExtractFilePath(ParamStr(0)) + 'models\';
  {$ENDIF}

  {$IFDEF LINUX}
  Result := GetEnvironmentVariable('HOME') + '/.onnx_app/models/';
  {$ENDIF}

  {$IFDEF DARWIN}
  Result := GetEnvironmentVariable('HOME') + '/Library/Application Support/ONNXApp/models/';
  {$ENDIF}
end;

procedure EnsureDirectories;
var
  modelsPath: string;
begin
  modelsPath := GetModelsPath;

  if not DirectoryExists(modelsPath) then
    ForceDirectories(modelsPath);
end;

end.
```

### Script de d√©ploiement

**Windows (deploy_windows.bat) :**
```batch
@echo off
echo D√©ploiement Windows...

REM Copier l'ex√©cutable
copy /Y MyApp.exe deploy\windows\

REM Copier ONNX Runtime
copy /Y C:\onnxruntime\bin\onnxruntime.dll deploy\windows\

REM Copier les mod√®les
xcopy /Y /E models\*.onnx deploy\windows\models\

echo D√©ploiement termin√©!
pause
```

**Linux (deploy_linux.sh) :**
```bash
#!/bin/bash
echo "D√©ploiement Linux..."

# Cr√©er la structure
mkdir -p deploy/linux/models

# Copier l'ex√©cutable
cp MyApp deploy/linux/

# Copier ONNX Runtime (si local)
if [ -f "lib/libonnxruntime.so" ]; then
    cp lib/libonnxruntime.so deploy/linux/
fi

# Copier les mod√®les
cp models/*.onnx deploy/linux/models/

# Cr√©er un script de lancement
cat > deploy/linux/run.sh << 'EOF'
#!/bin/bash
export LD_LIBRARY_PATH=.:$LD_LIBRARY_PATH
./MyApp
EOF

chmod +x deploy/linux/run.sh
chmod +x deploy/linux/MyApp

echo "D√©ploiement termin√©!"
```

---

## Application Compl√®te : D√©tecteur d'Objets

### Interface Lazarus

```pascal
unit MainForm;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, Forms, Controls, Graphics, Dialogs, ExtCtrls,
  StdCtrls, ComCtrls, Menus, ONNXInference;

type
  TFormMain = class(TForm)
    ButtonLoadModel: TButton;
    ButtonLoadImage: TButton;
    ButtonDetect: TButton;
    ImageOriginal: TImage;
    ImageDetected: TImage;
    LabelStatus: TLabel;
    ListBoxResults: TListBox;
    ProgressBar1: TProgressBar;
    OpenDialogModel: TOpenDialog;
    OpenDialogImage: TOpenDialog;
    PanelControls: TPanel;

    procedure FormCreate(Sender: TObject);
    procedure FormDestroy(Sender: TObject);
    procedure ButtonLoadModelClick(Sender: TObject);
    procedure ButtonLoadImageClick(Sender: TObject);
    procedure ButtonDetectClick(Sender: TObject);
  private
    FModel: TONNXModel;
    FModelLoaded: Boolean;
    FImagePath: string;

    procedure UpdateStatus(const AMessage: string);
    procedure DrawDetections(const ADetections: TArray<TDetection>);
  public
  end;

type
  TDetection = record
    X, Y, Width, Height: Integer;
    Confidence: Single;
    ClassName: string;
  end;

var
  FormMain: TFormMain;

implementation

{$R *.lfm}

procedure TFormMain.FormCreate(Sender: TObject);
begin
  FModel := TONNXModel.Create;
  FModelLoaded := False;

  ButtonDetect.Enabled := False;

  UpdateStatus('Pr√™t. Chargez un mod√®le pour commencer.');
end;

procedure TFormMain.FormDestroy(Sender: TObject);
begin
  FModel.Free;
end;

procedure TFormMain.UpdateStatus(const AMessage: string);
begin
  LabelStatus.Caption := AMessage;
  Application.ProcessMessages;
end;

procedure TFormMain.ButtonLoadModelClick(Sender: TObject);
begin
  OpenDialogModel.Filter := 'Mod√®les ONNX|*.onnx';

  if OpenDialogModel.Execute then
  begin
    UpdateStatus('Chargement du mod√®le...');
    ProgressBar1.Position := 30;

    if FModel.LoadModel(OpenDialogModel.FileName) then
    begin
      FModelLoaded := True;
      UpdateStatus('Mod√®le charg√©: ' + ExtractFileName(OpenDialogModel.FileName));
      ProgressBar1.Position := 100;
    end
    else
    begin
      FModelLoaded := False;
      UpdateStatus('√âchec du chargement du mod√®le');
      ProgressBar1.Position := 0;
      ShowMessage('Impossible de charger le mod√®le ONNX');
    end;
  end;
end;

procedure TFormMain.ButtonLoadImageClick(Sender: TObject);
begin
  OpenDialogImage.Filter := 'Images|*.bmp;*.jpg;*.jpeg;*.png';

  if OpenDialogImage.Execute then
  begin
    FImagePath := OpenDialogImage.FileName;

    try
      ImageOriginal.Picture.LoadFromFile(FImagePath);
      UpdateStatus('Image charg√©e: ' + ExtractFileName(FImagePath));

      ButtonDetect.Enabled := FModelLoaded;
    except
      on E: Exception do
      begin
        ShowMessage('Erreur de chargement: ' + E.Message);
        UpdateStatus('Erreur lors du chargement de l''image');
      end;
    end;
  end;
end;

procedure TFormMain.ButtonDetectClick(Sender: TObject);
var
  inputData, output: TArray<Single>;
  detections: TArray<TDetection>;
  i: Integer;
begin
  if not FModelLoaded or (FImagePath = '') then
  begin
    ShowMessage('Veuillez d''abord charger un mod√®le et une image');
    Exit;
  end;

  UpdateStatus('D√©tection en cours...');
  ListBoxResults.Clear;

  try
    // Pr√©traiter l'image
    inputData := PreprocessImage(ImageOriginal.Picture.Bitmap);

    // Faire l'inf√©rence
    output := FModel.Predict(inputData);

    // Post-traiter les r√©sultats
    detections := ParseDetections(output);

    // Afficher les r√©sultats
    ListBoxResults.Items.Add(Format('Objets d√©tect√©s: %d', [Length(detections)]));
    ListBoxResults.Items.Add('');

    for i := 0 to High(detections) do
    begin
      ListBoxResults.Items.Add(
        Format('%s - %.1f%% confiance',
          [detections[i].ClassName, detections[i].Confidence * 100])
      );
    end;

    // Dessiner les bo√Ætes de d√©tection
    DrawDetections(detections);

    UpdateStatus(Format('D√©tection termin√©e: %d objet(s) trouv√©(s)',
      [Length(detections)]));

  except
    on E: Exception do
    begin
      ShowMessage('Erreur lors de la d√©tection: ' + E.Message);
      UpdateStatus('Erreur de d√©tection');
    end;
  end;
end;

procedure TFormMain.DrawDetections(const ADetections: TArray<TDetection>);
var
  bitmap: TBitmap;
  i: Integer;
  rect: TRect;
begin
  // Copier l'image originale
  bitmap := TBitmap.Create;
  try
    bitmap.Assign(ImageOriginal.Picture.Bitmap);

    // Dessiner les bo√Ætes
    bitmap.Canvas.Pen.Color := clLime;
    bitmap.Canvas.Pen.Width := 3;
    bitmap.Canvas.Brush.Style := bsClear;
    bitmap.Canvas.Font.Color := clLime;
    bitmap.Canvas.Font.Size := 10;
    bitmap.Canvas.Font.Style := [fsBold];

    for i := 0 to High(ADetections) do
    begin
      rect := Rect(
        ADetections[i].X,
        ADetections[i].Y,
        ADetections[i].X + ADetections[i].Width,
        ADetections[i].Y + ADetections[i].Height
      );

      bitmap.Canvas.Rectangle(rect);
      bitmap.Canvas.TextOut(
        rect.Left + 5,
        rect.Top + 5,
        Format('%s %.0f%%',
          [ADetections[i].ClassName, ADetections[i].Confidence * 100])
      );
    end;

    ImageDetected.Picture.Assign(bitmap);

  finally
    bitmap.Free;
  end;
end;

end.
```

---

## Conversion de Mod√®les

### De TensorFlow vers ONNX

```python
# convert_tensorflow_to_onnx.py
import tensorflow as tf
import tf2onnx

# Charger un mod√®le TensorFlow
model = tf.keras.models.load_model('my_model.h5')

# Convertir en ONNX
spec = (tf.TensorSpec((None, 224, 224, 3), tf.float32, name="input"),)
output_path = "model.onnx"

model_proto, _ = tf2onnx.convert.from_keras(
    model,
    input_signature=spec,
    output_path=output_path
)

print(f"Mod√®le converti: {output_path}")
```

### De scikit-learn vers ONNX

```python
# convert_sklearn_to_onnx.py
from sklearn.ensemble import RandomForestClassifier
from skl2onnx import convert_sklearn
from skl2onnx.common.data_types import FloatTensorType

# Entra√Æner un mod√®le scikit-learn
model = RandomForestClassifier(n_estimators=10)
model.fit(X_train, y_train)

# D√©finir les types d'entr√©e
initial_type = [('float_input', FloatTensorType([None, 4]))]

# Convertir en ONNX
onx = convert_sklearn(model, initial_types=initial_type)

# Sauvegarder
with open("rf_model.onnx", "wb") as f:
    f.write(onx.SerializeToString())

print("Mod√®le RandomForest converti en ONNX!")
```

### V√©rifier un mod√®le ONNX

```python
# verify_onnx_model.py
import onnx

model = onnx.load("model.onnx")

# V√©rifier la validit√©
onnx.checker.check_model(model)
print("‚úì Mod√®le valide")

# Afficher les informations
print(f"Nom du mod√®le: {model.graph.name}")
print(f"Producteur: {model.producer_name}")
print(f"Version: {model.model_version}")

print("\nEntr√©es:")
for input in model.graph.input:
    print(f"  - {input.name}: {input.type}")

print("\nSorties:")
for output in model.graph.output:
    print(f"  - {output.name}: {output.type}")
```

---

## D√©bogage et R√©solution de Probl√®mes

### Probl√®mes courants

**1. Biblioth√®que ONNX Runtime introuvable**

```pascal
procedure CheckONNXInstallation;
var
  paths: array[0..2] of string;
  i: Integer;
  found: Boolean;
begin
  {$IFDEF WINDOWS}
  paths[0] := 'C:\onnxruntime\bin\onnxruntime.dll';
  paths[1] := ExtractFilePath(ParamStr(0)) + 'onnxruntime.dll';
  paths[2] := 'onnxruntime.dll';
  {$ENDIF}

  {$IFDEF LINUX}
  paths[0] := '/usr/local/lib/libonnxruntime.so';
  paths[1] := '/usr/lib/libonnxruntime.so';
  paths[2] := 'libonnxruntime.so';
  {$ENDIF}

  found := False;
  WriteLn('Recherche d''ONNX Runtime...');

  for i := 0 to High(paths) do
  begin
    Write('  Essai: ', paths[i], ' ... ');
    if FileExists(paths[i]) or (LoadLibrary(paths[i]) <> 0) then
    begin
      WriteLn('‚úì Trouv√©!');
      found := True;
      Break;
    end
    else
      WriteLn('‚úó');
  end;

  if not found then
  begin
    WriteLn;
    WriteLn('ONNX Runtime non trouv√©!');
    WriteLn('T√©l√©chargez depuis: https://github.com/microsoft/onnxruntime/releases');
  end;
end;
```

**2. Erreur de forme (shape mismatch)**

```pascal
procedure PrintModelInfo(const AModelPath: string);
begin
  WriteLn('Informations du mod√®le:');
  WriteLn('  Entr√©es attendues:');
  WriteLn('    - Nom: image');
  WriteLn('    - Forme: [1, 3, 224, 224]');
  WriteLn('    - Type: float32');
  WriteLn;
  WriteLn('V√©rifiez que vos donn√©es d''entr√©e correspondent!');
end;
```

**3. Performance lente**

```pascal
procedure ProfileInference(AModel: TONNXModel; AIterations: Integer);
var
  i: Integer;
  startTime, endTime: QWord;
  totalTime: Double;
  input: TArray<Single>;
begin
  SetLength(input, 3 * 224 * 224);

  // Warm-up
  AModel.Predict(input);

  startTime := GetTickCount64;

  for i := 1 to AIterations do
    AModel.Predict(input);

  endTime := GetTickCount64;
  totalTime := (endTime - startTime) / 1000.0;

  WriteLn(Format('Temps total: %.2f secondes', [totalTime]));
  WriteLn(Format('Temps moyen: %.0f ms par inf√©rence',
    [totalTime * 1000 / AIterations]));
  WriteLn(Format('D√©bit: %.1f images/seconde', [AIterations / totalTime]));
end;
```

---

## Ressources et Documentation

### Documentation officielle

üìö **ONNX**
- Site officiel: https://onnx.ai/
- GitHub: https://github.com/onnx/onnx
- Model Zoo: https://github.com/onnx/models

üìö **ONNX Runtime**
- Documentation: https://onnxruntime.ai/
- GitHub: https://github.com/microsoft/onnxruntime
- API C: https://onnxruntime.ai/docs/api/c/

### Outils utiles

üõ†Ô∏è **Netron** - Visualiseur de mod√®les
- https://netron.app/
- Permet de voir la structure d'un mod√®le ONNX
- Tr√®s utile pour d√©boguer

üõ†Ô∏è **ONNX Simplifier**
```bash
pip install onnx-simplifier
python -m onnxsim input.onnx output.onnx
```

üõ†Ô∏è **ONNX Optimizer**
```python
import onnx
from onnx import optimizer

model = onnx.load("model.onnx")
optimized_model = optimizer.optimize(model)
onnx.save(optimized_model, "model_optimized.onnx")
```

### Mod√®les pr√©-entra√Æn√©s

**O√π trouver des mod√®les :**

üåê **ONNX Model Zoo**
- https://github.com/onnx/models
- Mod√®les officiels v√©rifi√©s
- Vision, NLP, Audio, etc.

ü§ó **Hugging Face**
- https://huggingface.co/models
- Filtrer par "onnx" dans les tags
- Milliers de mod√®les pr√©-entra√Æn√©s

üéØ **TensorFlow Hub**
- https://tfhub.dev/
- Convertir en ONNX avec tf2onnx

üî• **PyTorch Hub**
- https://pytorch.org/hub/
- Exporter facilement en ONNX

### Benchmarks et Comparaisons

**Performance ONNX Runtime vs autres frameworks :**

| Framework | CPU (ms) | GPU (ms) | Taille (MB) |
|-----------|----------|----------|-------------|
| PyTorch | 45 | 12 | 250 |
| TensorFlow | 42 | 14 | 280 |
| **ONNX Runtime** | **28** | **8** | **90** |
| TensorRT | 25 | 6 | 100 |

*Benchmark ResNet-50 sur image 224√ó224*

---

## Cas d'Usage Avanc√©s

### 1. Pipeline complet de production

```pascal
unit ProductionPipeline;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, ONNXInference, Graphics, syncobjs;

type
  TModelCache = class
  private
    FModels: TThreadList;
    FCriticalSection: TCriticalSection;
  public
    constructor Create;
    destructor Destroy; override;

    function GetModel(const AModelName: string): TONNXModel;
    procedure ReleaseModel(AModel: TONNXModel);
  end;

  TInferenceRequest = record
    RequestID: string;
    ImagePath: string;
    Priority: Integer;
    Timestamp: TDateTime;
  end;

  TInferenceResult = record
    RequestID: string;
    Predictions: TStringList;
    Confidence: array of Single;
    ProcessingTime: Integer;
    Success: Boolean;
    ErrorMessage: string;
  end;

  TInferencePipeline = class
  private
    FModelCache: TModelCache;
    FRequestQueue: TThreadList;
    FResultQueue: TThreadList;
    FWorkerThreads: array of TThread;
    FRunning: Boolean;

    procedure ProcessRequest(ARequest: TInferenceRequest);
  public
    constructor Create(ANumWorkers: Integer = 4);
    destructor Destroy; override;

    procedure Start;
    procedure Stop;
    function SubmitRequest(const ARequest: TInferenceRequest): string;
    function GetResult(const ARequestID: string; out AResult: TInferenceResult): Boolean;

    property Running: Boolean read FRunning;
  end;

implementation

{ TModelCache }

constructor TModelCache.Create;
begin
  FModels := TThreadList.Create;
  FCriticalSection := TCriticalSection.Create;
end;

destructor TModelCache.Destroy;
var
  list: TList;
  i: Integer;
begin
  list := FModels.LockList;
  try
    for i := 0 to list.Count - 1 do
      TONNXModel(list[i]).Free;
    list.Clear;
  finally
    FModels.UnlockList;
  end;

  FModels.Free;
  FCriticalSection.Free;
  inherited;
end;

function TModelCache.GetModel(const AModelName: string): TONNXModel;
var
  list: TList;
  i: Integer;
  model: TONNXModel;
begin
  FCriticalSection.Enter;
  try
    list := FModels.LockList;
    try
      // Chercher un mod√®le disponible
      for i := 0 to list.Count - 1 do
      begin
        model := TONNXModel(list[i]);
        // TODO: V√©rifier si le mod√®le est disponible
        Result := model;
        Exit;
      end;

      // Cr√©er un nouveau mod√®le si n√©cessaire
      model := TONNXModel.Create;
      if model.LoadModel(AModelName) then
      begin
        list.Add(model);
        Result := model;
      end
      else
      begin
        model.Free;
        Result := nil;
      end;

    finally
      FModels.UnlockList;
    end;
  finally
    FCriticalSection.Leave;
  end;
end;

procedure TModelCache.ReleaseModel(AModel: TONNXModel);
begin
  // Le mod√®le retourne dans le pool
  // TODO: Impl√©menter la gestion du pool
end;

{ TInferencePipeline }

constructor TInferencePipeline.Create(ANumWorkers: Integer);
begin
  FModelCache := TModelCache.Create;
  FRequestQueue := TThreadList.Create;
  FResultQueue := TThreadList.Create;
  FRunning := False;

  SetLength(FWorkerThreads, ANumWorkers);
  // TODO: Cr√©er les threads workers
end;

destructor TInferencePipeline.Destroy;
begin
  Stop;
  FModelCache.Free;
  FRequestQueue.Free;
  FResultQueue.Free;
  inherited;
end;

procedure TInferencePipeline.Start;
begin
  FRunning := True;
  // TODO: D√©marrer les threads workers
end;

procedure TInferencePipeline.Stop;
begin
  FRunning := False;
  // TODO: Attendre la fin des threads
end;

function TInferencePipeline.SubmitRequest(const ARequest: TInferenceRequest): string;
var
  list: TList;
begin
  Result := ARequest.RequestID;

  list := FRequestQueue.LockList;
  try
    // Ajouter la requ√™te √† la queue
    // TODO: Impl√©menter avec une vraie structure de donn√©es
  finally
    FRequestQueue.UnlockList;
  end;
end;

function TInferencePipeline.GetResult(const ARequestID: string;
                                       out AResult: TInferenceResult): Boolean;
var
  list: TList;
begin
  Result := False;

  list := FResultQueue.LockList;
  try
    // Chercher le r√©sultat
    // TODO: Impl√©menter la recherche
  finally
    FResultQueue.UnlockList;
  end;
end;

procedure TInferencePipeline.ProcessRequest(ARequest: TInferenceRequest);
var
  model: TONNXModel;
  startTime: QWord;
  result: TInferenceResult;
begin
  startTime := GetTickCount64;

  model := FModelCache.GetModel('production_model.onnx');
  if model = nil then
  begin
    result.Success := False;
    result.ErrorMessage := 'Mod√®le non disponible';
    Exit;
  end;

  try
    // Traiter l'image
    // TODO: Impl√©menter le traitement

    result.Success := True;
    result.ProcessingTime := GetTickCount64 - startTime;

  finally
    FModelCache.ReleaseModel(model);
  end;
end;

end.
```

### 2. Service Web REST pour inf√©rence

```pascal
program ONNXWebService;

{$mode objfpc}{$H+}

uses
  {$IFDEF UNIX}
  cthreads,
  {$ENDIF}
  Classes, SysUtils, fphttpapp, httpdefs, httproute,
  fpjson, jsonparser, ONNXInference;

type
  TInferenceService = class
  private
    FModel: TONNXModel;
  public
    constructor Create;
    destructor Destroy; override;

    procedure HandlePredict(ARequest: TRequest; AResponse: TResponse);
    procedure HandleStatus(ARequest: TRequest; AResponse: TResponse);
  end;

var
  Service: TInferenceService;

constructor TInferenceService.Create;
begin
  FModel := TONNXModel.Create;

  if not FModel.LoadModel('model.onnx') then
    raise Exception.Create('√âchec du chargement du mod√®le');
end;

destructor TInferenceService.Destroy;
begin
  FModel.Free;
  inherited;
end;

procedure TInferenceService.HandlePredict(ARequest: TRequest; AResponse: TResponse);
var
  jsonData: TJSONObject;
  inputArray: TJSONArray;
  input: TArray<Single>;
  output: TArray<Single>;
  result: TJSONObject;
  i: Integer;
begin
  AResponse.ContentType := 'application/json';

  try
    // Parser la requ√™te JSON
    jsonData := TJSONObject(GetJSON(ARequest.Content));
    try
      inputArray := jsonData.Get('input', TJSONArray(nil));

      if inputArray = nil then
      begin
        AResponse.Code := 400;
        AResponse.Content := '{"error": "Missing input"}';
        Exit;
      end;

      // Convertir en tableau
      SetLength(input, inputArray.Count);
      for i := 0 to inputArray.Count - 1 do
        input[i] := inputArray.Floats[i];

      // Faire la pr√©diction
      output := FModel.Predict(input);

      // Construire la r√©ponse
      result := TJSONObject.Create;
      try
        result.Add('success', True);
        result.Add('output', TJSONArray.Create);

        for i := 0 to High(output) do
          TJSONArray(result.Arrays['output']).Add(output[i]);

        AResponse.Code := 200;
        AResponse.Content := result.AsJSON;

      finally
        result.Free;
      end;

    finally
      jsonData.Free;
    end;

  except
    on E: Exception do
    begin
      AResponse.Code := 500;
      AResponse.Content := Format('{"error": "%s"}', [E.Message]);
    end;
  end;
end;

procedure TInferenceService.HandleStatus(ARequest: TRequest; AResponse: TResponse);
var
  status: TJSONObject;
begin
  AResponse.ContentType := 'application/json';

  status := TJSONObject.Create;
  try
    status.Add('status', 'running');
    status.Add('model_loaded', FModel.Loaded);
    status.Add('version', '1.0.0');

    AResponse.Code := 200;
    AResponse.Content := status.AsJSON;

  finally
    status.Free;
  end;
end;

begin
  WriteLn('=== Service Web ONNX ===');
  WriteLn('D√©marrage...');

  try
    Service := TInferenceService.Create;
    try
      // Configurer les routes
      HTTPRouter.RegisterRoute('/predict', rmPost, @Service.HandlePredict);
      HTTPRouter.RegisterRoute('/status', rmGet, @Service.HandleStatus);

      // D√©marrer le serveur
      Application.Port := 8080;
      Application.Threaded := True;

      WriteLn('‚úì Serveur d√©marr√© sur le port 8080');
      WriteLn('Endpoints:');
      WriteLn('  POST /predict - Faire une pr√©diction');
      WriteLn('  GET  /status  - V√©rifier le statut');
      WriteLn;
      WriteLn('Appuyez sur Ctrl+C pour arr√™ter');

      Application.Run;

    finally
      Service.Free;
    end;

  except
    on E: Exception do
    begin
      WriteLn('Erreur: ', E.Message);
      ExitCode := 1;
    end;
  end;
end.
```

**Tester le service :**
```bash
# V√©rifier le statut
curl http://localhost:8080/status

# Faire une pr√©diction
curl -X POST http://localhost:8080/predict \
  -H "Content-Type: application/json" \
  -d '{"input": [1.0, 2.0, 3.0]}'
```

### 3. Monitoring et Logging

```pascal
unit ONNXMonitoring;

{$mode objfpc}{$H+}

interface

uses
  Classes, SysUtils, DateUtils;

type
  TInferenceMetrics = record
    TotalRequests: Int64;
    SuccessfulRequests: Int64;
    FailedRequests: Int64;
    AverageLatency: Double;
    MinLatency: Integer;
    MaxLatency: Integer;
    RequestsPerSecond: Double;
  end;

  TMetricsCollector = class
  private
    FMetrics: TInferenceMetrics;
    FStartTime: TDateTime;
    FLatencies: array of Integer;
    FLogFile: TextFile;
  public
    constructor Create(const ALogFile: string);
    destructor Destroy; override;

    procedure RecordRequest(ALatency: Integer; ASuccess: Boolean);
    function GetMetrics: TInferenceMetrics;
    procedure PrintMetrics;
    procedure ExportPrometheus(const AFileName: string);
  end;

implementation

constructor TMetricsCollector.Create(const ALogFile: string);
begin
  FStartTime := Now;

  FillChar(FMetrics, SizeOf(FMetrics), 0);
  FMetrics.MinLatency := MaxInt;

  SetLength(FLatencies, 0);

  AssignFile(FLogFile, ALogFile);
  Rewrite(FLogFile);
  WriteLn(FLogFile, 'timestamp,latency_ms,success');
end;

destructor TMetricsCollector.Destroy;
begin
  CloseFile(FLogFile);
  SetLength(FLatencies, 0);
  inherited;
end;

procedure TMetricsCollector.RecordRequest(ALatency: Integer; ASuccess: Boolean);
var
  elapsed: Double;
begin
  Inc(FMetrics.TotalRequests);

  if ASuccess then
    Inc(FMetrics.SuccessfulRequests)
  else
    Inc(FMetrics.FailedRequests);

  // Enregistrer la latence
  SetLength(FLatencies, Length(FLatencies) + 1);
  FLatencies[High(FLatencies)] := ALatency;

  if ALatency < FMetrics.MinLatency then
    FMetrics.MinLatency := ALatency;
  if ALatency > FMetrics.MaxLatency then
    FMetrics.MaxLatency := ALatency;

  // Calculer la moyenne
  FMetrics.AverageLatency :=
    (FMetrics.AverageLatency * (FMetrics.TotalRequests - 1) + ALatency) /
    FMetrics.TotalRequests;

  // Calculer les requ√™tes par seconde
  elapsed := SecondsBetween(Now, FStartTime);
  if elapsed > 0 then
    FMetrics.RequestsPerSecond := FMetrics.TotalRequests / elapsed;

  // Logger
  WriteLn(FLogFile, Format('%s,%d,%s',
    [FormatDateTime('yyyy-mm-dd hh:nn:ss', Now),
     ALatency,
     BoolToStr(ASuccess, True)]));
  Flush(FLogFile);
end;

function TMetricsCollector.GetMetrics: TInferenceMetrics;
begin
  Result := FMetrics;
end;

procedure TMetricsCollector.PrintMetrics;
begin
  WriteLn('=== M√©triques d''inf√©rence ===');
  WriteLn(Format('Requ√™tes totales: %d', [FMetrics.TotalRequests]));
  WriteLn(Format('  - R√©ussies: %d (%.1f%%)',
    [FMetrics.SuccessfulRequests,
     (FMetrics.SuccessfulRequests / FMetrics.TotalRequests * 100)]));
  WriteLn(Format('  - √âchou√©es: %d (%.1f%%)',
    [FMetrics.FailedRequests,
     (FMetrics.FailedRequests / FMetrics.TotalRequests * 100)]));
  WriteLn;
  WriteLn('Latence (ms):');
  WriteLn(Format('  - Moyenne: %.2f', [FMetrics.AverageLatency]));
  WriteLn(Format('  - Min: %d', [FMetrics.MinLatency]));
  WriteLn(Format('  - Max: %d', [FMetrics.MaxLatency]));
  WriteLn;
  WriteLn(Format('D√©bit: %.2f requ√™tes/seconde', [FMetrics.RequestsPerSecond]));
  WriteLn('=============================');
end;

procedure TMetricsCollector.ExportPrometheus(const AFileName: string);
var
  f: TextFile;
begin
  AssignFile(f, AFileName);
  try
    Rewrite(f);

    WriteLn(f, '# HELP inference_requests_total Total number of inference requests');
    WriteLn(f, '# TYPE inference_requests_total counter');
    WriteLn(f, Format('inference_requests_total %d', [FMetrics.TotalRequests]));
    WriteLn(f);

    WriteLn(f, '# HELP inference_latency_ms Average inference latency in milliseconds');
    WriteLn(f, '# TYPE inference_latency_ms gauge');
    WriteLn(f, Format('inference_latency_ms %.2f', [FMetrics.AverageLatency]));
    WriteLn(f);

    WriteLn(f, '# HELP inference_rate Inference requests per second');
    WriteLn(f, '# TYPE inference_rate gauge');
    WriteLn(f, Format('inference_rate %.2f', [FMetrics.RequestsPerSecond]));

  finally
    CloseFile(f);
  end;
end;

end.
```

---

## S√©curit√© et Bonnes Pratiques

### 1. Validation des entr√©es

```pascal
function ValidateInput(const AInput: TArray<Single>;
                       AExpectedSize: Integer): Boolean;
begin
  Result := False;

  // V√©rifier la taille
  if Length(AInput) <> AExpectedSize then
  begin
    WriteLn('Erreur: Taille d''entr√©e invalide');
    Exit;
  end;

  // V√©rifier les valeurs (pas de NaN, pas d'infini)
  for i := 0 to High(AInput) do
  begin
    if IsNan(AInput[i]) or IsInfinite(AInput[i]) then
    begin
      WriteLn('Erreur: Valeur invalide √† l''index ', i);
      Exit;
    end;
  end;

  Result := True;
end;
```

### 2. Gestion des erreurs robuste

```pascal
function SafePredict(AModel: TONNXModel;
                     const AInput: TArray<Single>): TArray<Single>;
var
  retryCount: Integer;
begin
  retryCount := 0;

  while retryCount < 3 do
  begin
    try
      Result := AModel.Predict(AInput);

      if Length(Result) > 0 then
        Exit; // Succ√®s

    except
      on E: Exception do
      begin
        Inc(retryCount);
        WriteLn(Format('Erreur (tentative %d/3): %s', [retryCount, E.Message]));

        if retryCount < 3 then
          Sleep(100); // Attendre avant de r√©essayer
      end;
    end;
  end;

  // √âchec apr√®s 3 tentatives
  SetLength(Result, 0);
end;
```

### 3. Limitation de ressources

```pascal
type
  TResourceLimiter = class
  private
    FMaxConcurrentRequests: Integer;
    FCurrentRequests: Integer;
    FLock: TCriticalSection;
  public
    constructor Create(AMaxRequests: Integer);
    destructor Destroy; override;

    function AcquireSlot: Boolean;
    procedure ReleaseSlot;
  end;

constructor TResourceLimiter.Create(AMaxRequests: Integer);
begin
  FMaxConcurrentRequests := AMaxRequests;
  FCurrentRequests := 0;
  FLock := TCriticalSection.Create;
end;

destructor TResourceLimiter.Destroy;
begin
  FLock.Free;
  inherited;
end;

function TResourceLimiter.AcquireSlot: Boolean;
begin
  FLock.Enter;
  try
    if FCurrentRequests < FMaxConcurrentRequests then
    begin
      Inc(FCurrentRequests);
      Result := True;
    end
    else
      Result := False;
  finally
    FLock.Leave;
  end;
end;

procedure TResourceLimiter.ReleaseSlot;
begin
  FLock.Enter;
  try
    if FCurrentRequests > 0 then
      Dec(FCurrentRequests);
  finally
    FLock.Leave;
  end;
end;
```

---

## Conclusion

### Ce que vous avez appris

Au cours de ce tutoriel, vous avez d√©couvert :

‚úÖ **Fondamentaux d'ONNX**
- Format universel pour les mod√®les ML
- Portabilit√© entre frameworks
- Avantages pour le d√©ploiement

‚úÖ **Installation et configuration**
- ONNX Runtime sur Windows/Linux
- Bindings FreePascal
- V√©rification de l'installation

‚úÖ **Utilisation pratique**
- Chargement de mod√®les
- Inf√©rence (pr√©dictions)
- Pr√©traitement des donn√©es
- Post-traitement des r√©sultats

‚úÖ **Applications concr√®tes**
- Classification d'images
- D√©tection d'objets
- Interface Lazarus
- Service Web REST

‚úÖ **Optimisation**
- Quantification
- Batch processing
- Support GPU
- Monitoring des performances

‚úÖ **Production**
- Pipeline complet
- Gestion d'erreurs
- M√©triques et logging
- S√©curit√©

### Avantages d'ONNX avec FreePascal

**Performance exceptionnelle**
- üöÄ Ex√©cution native optimis√©e
- üí® Pas d'overhead d'interpr√©teur
- ‚ö° Startup instantan√©

**D√©ploiement simplifi√©**
- üì¶ Un seul fichier .onnx
- üîß Pas de d√©pendances Python
- üíæ Empreinte m√©moire r√©duite

**Portabilit√© maximale**
- ü™ü Windows
- üêß Linux
- üçé macOS
- üì± Possible sur mobile (via ARM)

**Flexibilit√©**
- üîÑ Changer de mod√®le sans recompiler
- üéØ Utiliser des mod√®les de n'importe quel framework
- üõ†Ô∏è Int√©gration facile dans applications existantes

### Limitations et Solutions

**Limitations actuelles :**

‚ùå **Pas de training**
- ONNX est pour l'inf√©rence uniquement
- Solution : Entra√Æner avec PyTorch/TensorFlow, exporter en ONNX

‚ùå **Bindings incomplets**
- Toutes les fonctions ONNX Runtime ne sont pas expos√©es
- Solution : √âtendre les bindings selon vos besoins

‚ùå **Debugging complexe**
- Erreurs parfois cryptiques
- Solution : Utiliser Netron pour visualiser, valider avec onnx.checker

### Cas d'usage id√©aux

**‚úì Parfait pour :**
- Applications desktop avec IA
- Services d'inf√©rence haute performance
- Syst√®mes embarqu√©s avec contraintes
- D√©ploiement sur serveurs sans Python
- Applications n√©cessitant un d√©marrage rapide

**‚úó Moins adapt√© pour :**
- Entra√Ænement de mod√®les (utiliser PyTorch/TF)
- Recherche en ML (manque de flexibilit√©)
- Prototypage tr√®s rapide (Python plus simple)

### Prochaines √©tapes

**Pour aller plus loin :**

1. **Explorer d'autres types de mod√®les**
   - NLP (BERT, GPT)
   - Audio (Wav2Vec, DeepSpeech)
   - Multimodaux (CLIP, DALL-E)

2. **Optimisations avanc√©es**
   - TensorRT (NVIDIA)
   - OpenVINO (Intel)
   - Profiling d√©taill√©

3. **D√©ploiement √† grande √©chelle**
   - Kubernetes
   - Load balancing
   - Auto-scaling

4. **Am√©liorer les bindings**
   - API compl√®te ONNX Runtime
   - Support des nouveaux op√©rateurs
   - Wrapper orient√© objet avanc√©

### Ressources pour continuer

**üìö Apprentissage**
- ONNX Tutorials: https://github.com/onnx/tutorials
- ONNX Runtime Examples: https://github.com/microsoft/onnxruntime-inference-examples

**üõ†Ô∏è Outils**
- Netron (visualisation): https://netron.app/
- ONNX Converters: https://github.com/onnx/onnxmltools

**üí¨ Communaut√©**
- ONNX Slack: onnx-community.slack.com
- GitHub Discussions
- Stack Overflow (tag: onnx)

### Message final

ONNX avec FreePascal/Lazarus repr√©sente une combinaison puissante pour d√©ployer des mod√®les d'IA en production :

- **Entra√Ænez** avec les meilleurs outils (Python, PyTorch, TensorFlow)
- **Exportez** en format universel (ONNX)
- **D√©ployez** avec performance et simplicit√© (FreePascal)

Cette approche vous donne le **meilleur des deux mondes** :
- La richesse de l'√©cosyst√®me Python pour l'entra√Ænement
- La performance et la portabilit√© de FreePascal pour la production

**L'IA n'est plus r√©serv√©e aux data scientists !** Avec ONNX, tout d√©veloppeur FreePascal peut int√©grer des mod√®les d'apprentissage automatique de pointe dans ses applications.

Bon d√©ploiement et bonne inf√©rence ! üöÄü§ñ‚ú®

---

*Fin du tutoriel 15.8 - ONNX et mod√®les portables*

**Prochaine √©tape recommand√©e :** Combiner ONNX avec d'autres techniques (vision par ordinateur, traitement d'images) pour cr√©er des applications compl√®tes d'IA !

‚è≠Ô∏è [GPU computing avec CUDA/OpenCL](/15-intelligence-artificielle-machine-learning/09-gpu-computing-cuda-opencl.md)
